# 实验五：Transformer

## 开始之前

我们在本次实验中使用Jupyter Notebook进行实验，请参考[Jupyter Notebook使用](../lab1/环境配置指南.md#jupyter-notebook)，按照文档中详细步骤进行操作，完成Jupyter Notebook的配置。

## 实验任务
本次实验分为两个任务：Transformer（约90分钟）和Bert文本分类实验（约60分钟）。


### [任务一：Transformer](./transformer.md)
[下载任务一代码](transformer.ipynb){ .md-button}

在这个任务中，你将：

1. 了解位置编码
2. 了解多头自注意力机制原理与实现
3. 掌握Transformer编码器结构
4. 了解Transformer Encoder的堆叠与模块化设计

主要内容包括：

- 位置编码的基本原理
- 探索多头自注意力机制原理与实现
- 使用Transformer Encoder设计一个文本分类模型
- 模型训练与评估
- 思考题讨论

通过本任务，你将掌握位置编码的基本原理，了解多头自注意力机制原理与实现，并能够在PyTorch框架下使用Transformer Encoder设计训练一个文本分类模型。


### [任务二：RNN、LSTM和GRU文本生成任务](./transformer.md)
[下载任务二代码](embedding_main.ipynb){ .md-button}

在这个任务中，你将：

1. 实现文本数据集的预处理操作
2. 探索RNN、LSTM和GRU的差异

主要内容包括：

- 文本预处理
- RNN文本生成实验
- LSTM和GRU文本生成实验
- RNN、LSTM和GRU的比较思考

通过本任务，你将掌握文本预处理操作，理解RNN的、LSTM和GRU的差异，并能够根据实际情况选择合适的模型。

## 实验提交
实验完成后，请提交以下材料：

1. 运行成功的Jupyter Notebook文件
2. 一份PDF报告，报告内容**包括但不限于**：
    1. 任务中的填充代码和相应实验结果
    2. 实验五思考题答案
    3. 实验心得与体会

## 提交说明

1. 本次提交的内容为: 实验五要求提交的内容，具体要求请参见实验网站（https://zhiweinju.github.io/nju-dl-lab-2025spring/）
2. 提交的报告文件请以PDF文件格式上传到selearning网站，上传文件的文件命名格式为: 学号_姓名_实验五.pdf，比如：123456789_张三_实验五.pdf
3. 其他提交文件（如 运行成功的Jupyter Notebook文件），请加上前缀: 学号_姓名_，比如: 123456789_张三_lab5.ipynb
4. 本次提交的截止时间为 3月28日23:59:59
5. 对于迟交的处理: 迟交一周以内，折扣系数为0.8，迟交超过一周，折扣系数为0.6，超过一个月停止接收提交，尚未提交者本次作业计0分